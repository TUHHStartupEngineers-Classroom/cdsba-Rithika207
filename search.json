[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "# Load the file\nrandom_variable &lt;- readRDS(\"Causal_Data_Science_Data/random_vars.rds\")\n# Creating a Data Frame\ndf=data.frame(random_variable)\n\n\nFor each variable, Compute the following\n\n\n# 1.Mean \nmean(df$age)\n\n#&gt; [1] 33.471\n\nmean(df$income)\n\n#&gt; [1] 3510.731\n\n\n\n# 2. Variance\nvar(df$age)\n\n#&gt; [1] 340.6078\n\nvar(df$income)\n\n#&gt; [1] 8625646\n\n\n\n# 3. Standard deviation\nsd(df$age)\n\n#&gt; [1] 18.45556\n\nsd(df$income)\n\n#&gt; [1] 2936.945\n\n\n\nThe estimated Standard deviations are on wide range of scales. Because of this, comparing them makes no sense.\nCalculate Covariance and Correlation\n\n\n# Covariance\ncov(df)\n\n#&gt;               age     income\n#&gt; age      340.6078   29700.15\n#&gt; income 29700.1468 8625645.84\n\n\n\n# Correlation\ncor(df)\n\n#&gt;              age    income\n#&gt; age    1.0000000 0.5479432\n#&gt; income 0.5479432 1.0000000\n\n\n\nCorrelation is easy to interpret as it is a standardized measure. When age is increasing the income is also increasing as the correlation is a positive value.\nCompute the Conditional Expected value\n\n\n# 1\nage_18 &lt;- subset(df, age&lt;=18, select = c(age,income))\nmean(age_18$income)\n\n#&gt; [1] 389.6074\n\n\n\n# 2\nage_65 &lt;- subset(df, age&gt;=65, select = c(age,income))\nmean(age_65$income)\n\n#&gt; [1] 1777.237\n\n\n\n# 3\nage_18_65 &lt;- subset(df, age&gt;=18 & age&lt;65, select = c(age,income))\nmean(age_18_65$income)\n\n#&gt; [1] 4685.734"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "1 Assignment\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\n\n# Create two unrelated variables\nset.seed(789)\nx &lt;- rnorm(20)\ny &lt;- sin(x) + rnorm(20, sd = 0.2)\n\n\n# Create a dataframe\ndata &lt;- data.frame(x = x, y = y)\n\n\n# Create a scatter plot with a smooth line\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Spurious Correlation\",\n       x = \"Variable X\",\n       y = \"Variable Y\") +\n  theme_minimal()\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "# Load packages\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\n\n\n# Load the file\nmembership_data &lt;- readRDS(\"Causal_Data_Science_Data/membership.rds\")\n# Create a Data Frame\ndf=data.frame(membership_data)\n\n\nDraw DAG to understand the realtions between variables\n\n\n# Confounding variables are age, sex, prev_avg_purch\npurchase&lt;- dagify(\n  card  ~ age + sex + prev_avg_purch, sex ~ age , prev_avg_purch ~ sex, avg_purch ~ card ,\n  coords = list(x = c(age = 1,sex = 2, prev_avg_purch = 3, card = 1.5, avg_purch = 2.5),\n                      y = c(age = 1,sex = 1, prev_avg_purch = 1, card = 2, avg_purch = 2)  )\n)\n\nggdag(purchase, use_labels = \"name\", text = F) + theme_dag()\n\n\n\n\n\n\n\n\nThe relationship in the membership.rds is states as follows: Sales are explained by average purchases and they depend on the membership cards, but as a back door path, they also depend on age, sex or previous average purchase.\n\nNaive estimate:\n\n\nmodel_naive &lt;- lm(avg_purch ~ card, data = df)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Subclassification estimator (subclasses: Z = 0 and Z = 1)\n# E(Z, D)\nE_00 &lt;- mean(df[(df$sex==F & df$card==F), ]$avg_purch) \nE_10 &lt;- mean(df[(df$sex==T & df$card==F), ]$avg_purch) \nE_01 &lt;- mean(df[(df$sex==F & df$card==T), ]$avg_purch) \nE_11 &lt;- mean(df[(df$sex==T & df$card==T), ]$avg_purch) \n\n# Weighted by K (proportion of female/male)\nK &lt;- mean(df$sex)\n\nK*(E_11-E_10) + (1-K)*(E_01 - E_00)\n\n#&gt; [1] 25.22093\n\n\n\nCompute the following:\n\n3.1 Coarsened exact matching\n\n# Load 'MatchIt' library\nlibrary(MatchIt)\n\n# Without specifying coarsening\n# (1) Matching\ncem &lt;- matchit(card ~ age + pre_avg_purch,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch, data = df, method = \"cem\", \n#&gt;     estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.2869       40.2645          0.0017     1.0001    0.0016\n#&gt; pre_avg_purch       70.5402       70.1875          0.0135     0.9910    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0067          0.1224\n#&gt; pre_avg_purch   0.0133          0.1557\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.   4232.  \n#&gt; Matched (ESS) 5527.11 3928.04\n#&gt; Matched       5752.   4199.  \n#&gt; Unmatched       16.     33.  \n#&gt; Discarded        0.      0.\n\n# Use matched data\ndf_cem &lt;- match.data(cem)\n\n# (2) Estimation\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -155.31  -20.74   -0.17   20.25  146.91 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.0474     0.3996  175.30   &lt;2e-16 ***\n#&gt; card         15.2687     0.6151   24.82   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.31 on 9949 degrees of freedom\n#&gt; Multiple R-squared:  0.05831,    Adjusted R-squared:  0.05822 \n#&gt; F-statistic: 616.1 on 1 and 9949 DF,  p-value: &lt; 2.2e-16\n\n\n3.2 Nearest neighbour matching\n\n# (1) Matching\n\nnn &lt;- matchit(card ~ age + pre_avg_purch,\n              data = df,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              )\n\n# Covariate Balance\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch, data = df, method = \"nearest\", \n#&gt;     distance = \"mahalanobis\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.2938          0.0531     1.1056    0.0111\n#&gt; pre_avg_purch       76.3938       74.0433          0.0894     1.2062    0.0200\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0284          0.0772\n#&gt; pre_avg_purch   0.0633          0.1112\n#&gt; \n#&gt; Sample Sizes:\n#&gt;           Control Treated\n#&gt; All          5768    4232\n#&gt; Matched      4232    4232\n#&gt; Unmatched    1536       0\n#&gt; Discarded       0       0\n\n# Use matched data\ndf_nn &lt;- match.data(nn)\n\n# (2) Estimation\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -19.954    0.074   19.602  112.234 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  73.8717     0.4509  163.83   &lt;2e-16 ***\n#&gt; card         17.2875     0.6377   27.11   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 29.33 on 8462 degrees of freedom\n#&gt; Multiple R-squared:  0.07991,    Adjusted R-squared:  0.0798 \n#&gt; F-statistic: 734.9 on 1 and 8462 DF,  p-value: &lt; 2.2e-16\n\n\n3.3 Inverse probability weighting\n\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ age + pre_avg_purch,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4121735  0.0723729 -19.512   &lt;2e-16 ***\n#&gt; age            0.0011734  0.0017758   0.661    0.509    \n#&gt; pre_avg_purch  0.0148181  0.0009263  15.996   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13250  on 9997  degrees of freedom\n#&gt; AIC: 13256\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\n\n# Add propensities to table\ndf_aug &lt;- df %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\n\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\n# Look at data with IPW scores\ndf_ipw %&gt;% \n  select(card, age, pre_avg_purch , propensity, ipw)\n\n\n\n  \n\n\n\n\n# (2) Estimation\nmodel_ipw &lt;- lm(avg_purch ~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -203.886  -29.009   -0.273   28.782  215.682 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2627     0.4320  162.65   &lt;2e-16 ***\n#&gt; card         14.9548     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.2 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05654,    Adjusted R-squared:  0.05645 \n#&gt; F-statistic: 599.2 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# Load necessary Libraries\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\n\n\nDAG for Parking Spots\n\n\nParkingspots &lt;- dagify(\n  ParkingSpots ~ Location,\n  Sales ~ Location,\n  Sales ~ ParkingSpots,\n  coords = list(x = c(Sales = 3, Location = 2, ParkingSpots = 1),\n                y = c(Sales = 0, Location = 1, ParkingSpots = 0))\n)\n# Plot DAG\nggdag(Parkingspots, use_labels = \"name\", text =  F) + theme_dag()\n\n\n\n\n\n\n\n\nWhen the store is located within the city, no parking spots are required because customers approach the store directly and do not need to arrange their visit. As a result, the sales depend on the location. However when the store is located outside of the city, parking spots are required which I believe has a direct impact on sales.\n\nRegression\n\n\n# Load the file\ncar_prices &lt;- readRDS(\"Causal_Data_Science_Data/customer_sat.rds\")\n# Create a Data Frame\ndf1=data.frame(car_prices)\n\n2.1 Regress satisfaction on follow_ups\n\n#lm_sf is the regresssion of satisfaction on follow_ups\nlm_sf &lt;- lm(satisfaction ~ follow_ups , data = df1)\nsummary(lm_sf)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = df1)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n2.2 Regress satisfaction on follow_ups and account for subscription\n\n#lm_sfa is the regresssion of satisfaction on follow_ups and account for subscription\nlm_sfa &lt;- lm(satisfaction ~ follow_ups + subscription , data = df1)\nsummary(lm_sfa)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = df1)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\nComparing the coefficients:\n\nIn the first case, satisfaction on follow_ups are negatively correlated which suggests that the increase in satisfaction with the product and service will decrease the follow_up calls to the clients.\nIn the second case, satisfaction on follow_ups which accounts for subscription is highly correlated premium subscription levels than elite subscription levels.\n4 Plot the data\n\n# Not conditioning on subscription\nsubscription_not_cond &lt;- ggplot(df1, aes(x = satisfaction, y = follow_ups)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on subscription \nsubscription_cond &lt;- ggplot(df1, aes(x = satisfaction, y = follow_ups, color = subscription)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\n# Plot both plots\nsubscription_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsubscription_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\n\n\n# Load the file\nhospital_data &lt;- readRDS(\"Causal_Data_Science_Data/hospdd.rds\")\n# Create a Data Frame\ndf_hd=data.frame(hospital_data)\n\n\n# Convert month to a numeric variable\ndf_hd$month &lt;- as.numeric(as.character(df_hd$month))\n# Create a binary indicator for the post-treatment period\ndf_hd$treatment &lt;- ifelse(df_hd$month &gt;= 4, 1, 0)  # Assuming treatment occurred after month 3\n# Function to calculate mean manually\ncalculate_manual_mean &lt;- function(x) {\n  if (length(x) == 0) {\n    return(0)  # Return NA if there are no observations\n  }\n  sum_value &lt;- sum(x)\n  count_value &lt;- length(x)\n  return(sum_value / count_value)\n}\n\nmean_satisfaction_control_before &lt;- df_hd %&gt;%\n  filter(procedure == 0, treatment == 0) %&gt;%\n  pull(satis) %&gt;%\n  calculate_manual_mean()\n\nmean_satisfaction_treated_before &lt;- df_hd %&gt;%\n  filter(procedure == 1, treatment == 0) %&gt;%\n  pull(satis) %&gt;%\n  calculate_manual_mean()\n\nmean_satisfaction_control_after &lt;- df_hd %&gt;%\n  filter(procedure == 0, treatment == 1) %&gt;%\n  pull(satis) %&gt;%\n  calculate_manual_mean()\n\nmean_satisfaction_treated_after &lt;- df_hd %&gt;%\n  filter(procedure == 1, treatment == 1) %&gt;%\n  pull(satis) %&gt;%\n  calculate_manual_mean()\n\n# Print the results\ncat(\"Mean Satisfaction for Control Hospitals Before Treatment:\", mean_satisfaction_control_before, \"\\n\")\n\n#&gt; Mean Satisfaction for Control Hospitals Before Treatment: 3.447765\n\ncat(\"Mean Satisfaction for Treated Hospitals Before Treatment:\", mean_satisfaction_treated_before, \"\\n\")\n\n#&gt; Mean Satisfaction for Treated Hospitals Before Treatment: 0\n\ncat(\"Mean Satisfaction for Control Hospitals After Treatment:\", mean_satisfaction_control_after, \"\\n\")\n\n#&gt; Mean Satisfaction for Control Hospitals After Treatment: 3.38249\n\ncat(\"Mean Satisfaction for Treated Hospitals After Treatment:\", mean_satisfaction_treated_after, \"\\n\")\n\n#&gt; Mean Satisfaction for Treated Hospitals After Treatment: 4.363351\n\n\n\nUsing a linear regression to compute and estimate:\n\n\n# 2.1 Linear regression with month + hospital\nlm_mh &lt;- lm(procedure ~ month + hospital, data = df_hd)\nsummary(lm_mh)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = procedure ~ month + hospital, data = df_hd)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.56141 -0.23972 -0.04324  0.24191  0.68376 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.2501884  0.0088703   28.20   &lt;2e-16 ***\n#&gt; month        0.0869378  0.0016108   53.97   &lt;2e-16 ***\n#&gt; hospital    -0.0156497  0.0002513  -62.26   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.2928 on 7365 degrees of freedom\n#&gt; Multiple R-squared:  0.4797, Adjusted R-squared:  0.4795 \n#&gt; F-statistic:  3395 on 2 and 7365 DF,  p-value: &lt; 2.2e-16\n\n\n\n# 2.2 Linear regression as.factor(month) + as.factor(hospital)\nlm_f_mh &lt;- lm(procedure ~ as.factor(month) + as.factor(hospital), data = df_hd)\nsummary(lm_f_mh)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = procedure ~ as.factor(month) + as.factor(hospital), \n#&gt;     data = df_hd)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.2921 -0.2079  0.0000  0.2079  0.2921 \n#&gt; \n#&gt; Coefficients:\n#&gt;                         Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            2.921e-01  1.890e-02   15.45   &lt;2e-16 ***\n#&gt; as.factor(month)2     -5.435e-14  9.981e-03    0.00        1    \n#&gt; as.factor(month)3     -5.076e-15  9.981e-03    0.00        1    \n#&gt; as.factor(month)4      4.159e-01  9.981e-03   41.67   &lt;2e-16 ***\n#&gt; as.factor(month)5      4.159e-01  9.981e-03   41.67   &lt;2e-16 ***\n#&gt; as.factor(month)6      4.159e-01  9.981e-03   41.67   &lt;2e-16 ***\n#&gt; as.factor(month)7      4.159e-01  9.981e-03   41.67   &lt;2e-16 ***\n#&gt; as.factor(hospital)2   2.334e-14  2.639e-02    0.00        1    \n#&gt; as.factor(hospital)3   2.082e-15  2.711e-02    0.00        1    \n#&gt; as.factor(hospital)4   1.161e-14  2.526e-02    0.00        1    \n#&gt; as.factor(hospital)5   1.641e-14  2.526e-02    0.00        1    \n#&gt; as.factor(hospital)6   1.329e-14  2.526e-02    0.00        1    \n#&gt; as.factor(hospital)7  -3.935e-15  2.441e-02    0.00        1    \n#&gt; as.factor(hospital)8   2.745e-14  2.608e-02    0.00        1    \n#&gt; as.factor(hospital)9  -3.100e-14  2.673e-02    0.00        1    \n#&gt; as.factor(hospital)10  1.275e-14  2.639e-02    0.00        1    \n#&gt; as.factor(hospital)11 -9.250e-15  2.608e-02    0.00        1    \n#&gt; as.factor(hospital)12  3.702e-14  2.673e-02    0.00        1    \n#&gt; as.factor(hospital)13 -1.529e-14  2.578e-02    0.00        1    \n#&gt; as.factor(hospital)14 -4.284e-14  2.711e-02    0.00        1    \n#&gt; as.factor(hospital)15 -2.265e-14  2.711e-02    0.00        1    \n#&gt; as.factor(hospital)16  2.440e-14  2.639e-02    0.00        1    \n#&gt; as.factor(hospital)17  2.345e-14  2.752e-02    0.00        1    \n#&gt; as.factor(hospital)18 -1.045e-14  3.205e-02    0.00        1    \n#&gt; as.factor(hospital)19 -5.000e-01  2.711e-02  -18.45   &lt;2e-16 ***\n#&gt; as.factor(hospital)20 -5.000e-01  2.639e-02  -18.95   &lt;2e-16 ***\n#&gt; as.factor(hospital)21 -5.000e-01  2.797e-02  -17.88   &lt;2e-16 ***\n#&gt; as.factor(hospital)22 -5.000e-01  2.752e-02  -18.17   &lt;2e-16 ***\n#&gt; as.factor(hospital)23 -5.000e-01  2.711e-02  -18.45   &lt;2e-16 ***\n#&gt; as.factor(hospital)24 -5.000e-01  2.902e-02  -17.23   &lt;2e-16 ***\n#&gt; as.factor(hospital)25 -5.000e-01  3.114e-02  -16.06   &lt;2e-16 ***\n#&gt; as.factor(hospital)26 -5.000e-01  2.639e-02  -18.95   &lt;2e-16 ***\n#&gt; as.factor(hospital)27 -5.000e-01  2.551e-02  -19.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)28 -5.000e-01  2.797e-02  -17.88   &lt;2e-16 ***\n#&gt; as.factor(hospital)29 -5.000e-01  2.673e-02  -18.70   &lt;2e-16 ***\n#&gt; as.factor(hospital)30 -5.000e-01  3.205e-02  -15.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)31 -5.000e-01  2.639e-02  -18.95   &lt;2e-16 ***\n#&gt; as.factor(hospital)32 -5.000e-01  2.673e-02  -18.70   &lt;2e-16 ***\n#&gt; as.factor(hospital)33 -5.000e-01  2.639e-02  -18.95   &lt;2e-16 ***\n#&gt; as.factor(hospital)34 -5.000e-01  2.481e-02  -20.15   &lt;2e-16 ***\n#&gt; as.factor(hospital)35 -5.000e-01  2.551e-02  -19.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)36 -5.000e-01  2.578e-02  -19.39   &lt;2e-16 ***\n#&gt; as.factor(hospital)37 -5.000e-01  3.114e-02  -16.06   &lt;2e-16 ***\n#&gt; as.factor(hospital)38 -5.000e-01  2.608e-02  -19.18   &lt;2e-16 ***\n#&gt; as.factor(hospital)39 -5.000e-01  2.752e-02  -18.17   &lt;2e-16 ***\n#&gt; as.factor(hospital)40 -5.000e-01  2.608e-02  -19.18   &lt;2e-16 ***\n#&gt; as.factor(hospital)41 -5.000e-01  2.551e-02  -19.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)42 -5.000e-01  2.846e-02  -17.57   &lt;2e-16 ***\n#&gt; as.factor(hospital)43 -5.000e-01  2.711e-02  -18.45   &lt;2e-16 ***\n#&gt; as.factor(hospital)44 -5.000e-01  3.034e-02  -16.48   &lt;2e-16 ***\n#&gt; as.factor(hospital)45 -5.000e-01  2.551e-02  -19.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)46 -5.000e-01  2.752e-02  -18.17   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.2473 on 7316 degrees of freedom\n#&gt; Multiple R-squared:  0.6313, Adjusted R-squared:  0.6287 \n#&gt; F-statistic: 245.6 on 51 and 7316 DF,  p-value: &lt; 2.2e-16\n\n\nIn the case of factor function the regression is occurring with every hospital and all the 18 hospitals that introduced new admission procedure has a p-value of 1. So, a regression cannot be performed without this factor function’s help. The other difference is that the factor function gives detailed values of the estimates for all the months and all the hospitals whereas the normal regression is only confined to two coefficients."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "1 Assignment 1\n\n# Given probabilities\nP_T &lt;- 0.8\nP_T_NOT &lt;- 0.2\nP_S &lt;- 0.3\nP_S_NOT &lt;- 0.7\n\n# Calculating probabilities\nP_T_AND_S &lt;- P_T * P_S\nP_T_AND_S_NOT &lt;- P_T * P_S_NOT\nP_T_NOT_AND_S &lt;- P_T_NOT * P_S\nP_T_NOT_AND_S_NOT &lt;- P_T_NOT * P_S_NOT\n\n# Sum of probabilities\nsum_of_probabilites &lt;- P_T_AND_S + P_T_AND_S_NOT + P_T_NOT_AND_S + P_T_NOT_AND_S_NOT\n\n# Displaying the results\ncat(\"P(T ∩ S):\", P_T_AND_S, \"\\n\")\n\n#&gt; P(T ∩ S): 0.24\n\ncat(\"P(T ∩ S'): \", P_T_AND_S_NOT, \"\\n\")\n\n#&gt; P(T ∩ S'):  0.56\n\ncat(\"P(T' ∩ S):\", P_T_NOT_AND_S , \"\\n\")\n\n#&gt; P(T' ∩ S): 0.06\n\ncat(\"P(T' ∩ S'): \", P_T_NOT_AND_S_NOT, \"\\n\")\n\n#&gt; P(T' ∩ S'):  0.14\n\ncat(\"Sum of probabilities:\", sum_of_probabilites, \"\\n\")\n\n#&gt; Sum of probabilities: 1\n\n\n\n\n2 Assignment 2\n\n# Given probabilities\nP_A &lt;- 0.423\nP_B &lt;- 0.278\nP_C &lt;- 0.1\nP_A_intersect_B &lt;- 0.073\nP_B_intersect_C &lt;- 0.033\nP_A_intersect_C &lt;- 0.088\nP_A_intersect_B_intersect_C &lt;- 0.005\n\n# Calculate the percentage of customers using all three devices\nP_all_three &lt;- P_A_intersect_B_intersect_C * 100\n\n# Calculate the percentage of customers using at least two devices\nP_at_least_two &lt;- (P_A + P_B + P_C - P_A_intersect_B - P_B_intersect_C - P_A_intersect_C +\n                   P_A_intersect_B_intersect_C) * 100\n\n# Calculate the percentage of customers using only one device\nP_only_one &lt;- ((P_A - P_A_intersect_B - P_A_intersect_C + P_A_intersect_B_intersect_C) +\n               (P_B - P_A_intersect_B - P_B_intersect_C + P_A_intersect_B_intersect_C) +\n               (P_C - P_A_intersect_C - P_B_intersect_C + P_A_intersect_B_intersect_C)) * 100\n\n# Print results\ncat(\"Percentage of customers using all three devices:\", P_all_three, \"\\n\")\n\n#&gt; Percentage of customers using all three devices: 0.5\n\ncat(\"Percentage of customers using at least two devices:\", P_at_least_two, \"\\n\")\n\n#&gt; Percentage of customers using at least two devices: 61.2\n\ncat(\"Percentage of customers using only one device:\", P_only_one, \"\\n\")\n\n#&gt; Percentage of customers using only one device: 42.8\n\n\n\n# Given probabilities\nP_B_A &lt;- 0.97\nP_B_notA &lt;- 0.01\nP_A &lt;- 0.04\nnot_P_A &lt;- 1 - P_A\n\n# Calculate the probability B\nP_B &lt;- P_B_A * P_A + P_B_notA * not_P_A\ncat(\"Probability of B:\", P_B, \"\\n\")\n\n#&gt; Probability of B: 0.0484\n\n# Calculate the probability A|B using conditional probability formula\nP_A_B &lt;- P_B_A * P_A / P_B\ncat(\"Probability of P_A_B:\", P_A_B, \"\\n\")\n\n#&gt; Probability of P_A_B: 0.8016529\n\n# Calculate the probability not_A|B using conditional probability formula\nP_not_A_B &lt;- P_B_notA * not_P_A / P_B\ncat(\"Probability of P_not_A_B:\", P_not_A_B, \"\\n\")\n\n#&gt; Probability of P_not_A_B: 0.1983471\n\n\nThe following sentence: These results show that in case the alarm is triggered, there is a possibility of about 19.83% that the product is flawless and a probability of 80.16% that the product is faulty."
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "4.1 Header 2",
    "text": "4.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "1 Assignmnet\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ncar_data &lt;- readRDS(\"Causal_Data_Science_Data/car_prices.rds\")\ndf_1=data.frame(car_data)\n\n\n# 1\ndim(df_1)\n\n#&gt; [1] 181  22\n\n\nThere are 181 rows and 22 columns in the data file.\n\nUsing appropriate commands to get a more detailed look at the data.\n\n\nhead(df_1)\n\n\n\n  \n\n\n\n\nglimpse(df_1)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\n\n\nsummary(df_1)\n\n#&gt;   aspiration         doornumber          carbody           drivewheel       \n#&gt;  Length:181         Length:181         Length:181         Length:181        \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;  enginelocation       wheelbase        carlength        carwidth    \n#&gt;  Length:181         Min.   : 86.60   Min.   :141.1   Min.   :60.30  \n#&gt;  Class :character   1st Qu.: 94.50   1st Qu.:166.3   1st Qu.:64.00  \n#&gt;  Mode  :character   Median : 96.50   Median :173.0   Median :65.40  \n#&gt;                     Mean   : 98.21   Mean   :173.3   Mean   :65.74  \n#&gt;                     3rd Qu.:100.40   3rd Qu.:180.2   3rd Qu.:66.50  \n#&gt;                     Max.   :120.90   Max.   :208.1   Max.   :72.30  \n#&gt;    carheight       curbweight    enginetype        cylindernumber    \n#&gt;  Min.   :47.80   Min.   :1488   Length:181         Length:181        \n#&gt;  1st Qu.:52.00   1st Qu.:2122   Class :character   Class :character  \n#&gt;  Median :53.70   Median :2410   Mode  :character   Mode  :character  \n#&gt;  Mean   :53.58   Mean   :2521                                        \n#&gt;  3rd Qu.:55.50   3rd Qu.:2910                                        \n#&gt;  Max.   :59.80   Max.   :4066                                        \n#&gt;    enginesize     fuelsystem          boreratio         stroke    \n#&gt;  Min.   : 61.0   Length:181         Min.   :2.540   Min.   :2.07  \n#&gt;  1st Qu.: 98.0   Class :character   1st Qu.:3.150   1st Qu.:3.08  \n#&gt;  Median :120.0   Mode  :character   Median :3.310   Median :3.23  \n#&gt;  Mean   :127.1                      Mean   :3.325   Mean   :3.23  \n#&gt;  3rd Qu.:141.0                      3rd Qu.:3.590   3rd Qu.:3.40  \n#&gt;  Max.   :326.0                      Max.   :3.940   Max.   :4.17  \n#&gt;  compressionratio   horsepower       peakrpm        citympg     \n#&gt;  Min.   : 7.000   Min.   : 48.0   Min.   :4200   Min.   :13.00  \n#&gt;  1st Qu.: 8.500   1st Qu.: 70.0   1st Qu.:4800   1st Qu.:19.00  \n#&gt;  Median : 9.000   Median : 95.0   Median :5200   Median :24.00  \n#&gt;  Mean   : 8.848   Mean   :106.2   Mean   :5182   Mean   :24.85  \n#&gt;  3rd Qu.: 9.400   3rd Qu.:116.0   3rd Qu.:5500   3rd Qu.:30.00  \n#&gt;  Max.   :11.500   Max.   :288.0   Max.   :6600   Max.   :49.00  \n#&gt;    highwaympg        price      \n#&gt;  Min.   :16.00   Min.   : 5118  \n#&gt;  1st Qu.:25.00   1st Qu.: 7609  \n#&gt;  Median :30.00   Median : 9980  \n#&gt;  Mean   :30.48   Mean   :12999  \n#&gt;  3rd Qu.:34.00   3rd Qu.:16430  \n#&gt;  Max.   :54.00   Max.   :45400\n\n\nThe data types present are character and numeric data type.\n\nLinear regression\n\n\nlm_all_factors &lt;- lm(price ~ ., data = df_1)\nsummary(lm_all_factors)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = df_1)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nAll factors with a p-value less than 0.05 are relevant for the pricing of a car like carwidth, enginetypeohc, enginetypeohcv and enginesize so on.\n\nChoosing one regressor\n\n\nlm_one &lt;- lm(price ~ enginesize, data = df_1)\nsummary(lm_one)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ enginesize, data = df_1)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10818.6  -1969.6   -168.6   1494.0  14393.9 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -8622.296    873.535  -9.871   &lt;2e-16 ***\n#&gt; enginesize    170.064      6.523  26.073   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3694 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.7916, Adjusted R-squared:  0.7904 \n#&gt; F-statistic: 679.8 on 1 and 179 DF,  p-value: &lt; 2.2e-16\n\n\n4.1 The regressor used was enginesize and it belongs to the numeric variables (discrete/continous).\n4.2 As our estimate is positive (170.064), we have a positive effect. As a result increasing the enginesize would increase price of the car.\n4.3 Yes, it is satistically significant as the p value is lower than the significance level (0.05).\n\nAdd a variable seat_heating to the data\n\n\ndf_1 %&gt;% mutate(seatheating = TRUE)\n\n\n\n  \n\n\n\n\n# Changing the variables from logical variables to numerical variables\ndf_2&lt;- df_1 %&gt;% mutate(seatheating = 1)\ndf_2\n\n\n\n  \n\n\n\n\n# New regression\nlm_new_variable &lt;- lm(price ~ seatheating , data = df_2)\nsummary(lm_new_variable)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ seatheating, data = df_2)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -7881  -5390  -3019   3431  32401 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  12999.4      599.7   21.68   &lt;2e-16 ***\n#&gt; seatheating       NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8068 on 180 degrees of freedom\n\n\nThere is no relation between seat heating and pricing as there is no coefficient present in the new regression. If there is False value there might be coefficient value."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(dagitty)\n\n\n# Load the file\ncoupon&lt;- readRDS(\"Causal_Data_Science_Data/coupon.rds\")\n# Create a Data Frame\ndf1=data.frame(coupon)\n\n\n# Load the file\nshipping&lt;- readRDS(\"Causal_Data_Science_Data/shipping.rds\")\n# Create a Data Frame\ndf2=data.frame(shipping)\n\n\nWith half the bandwidth\n\n\n# Define cut-off\nc0 = 60\n# Half bandwidth\nbw &lt;- c0 + c(-2.5, 2.5)\n\n# Subsets below and above threshold in specified bandwidth\ndf1_bw_below &lt;- df1 %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf1_bw_above &lt;- df1 %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf1_bw &lt;- bind_rows(df1_bw_above, df1_bw_below)\ndim(df1_bw)\n\n#&gt; [1] 181   4\n\n\n\nwith double the bandwidth\n\n\n# Define cut-off\nc0 = 60\n# Double bandwidth\nbw &lt;- c0 + c(-10, 10)\n\n# Subsets below and above threshold in specified bandwidth\ndf1_bw_below &lt;- df1 %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf1_bw_above &lt;- df1 %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf1_bw &lt;- bind_rows(df1_bw_above, df1_bw_below)\ndim(df1_bw)\n\n#&gt; [1] 629   4\n\n\n\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nlibrary(rddensity)\nrddd &lt;- rddensity(df2$purchase_amount, c = 30)\nsummary(rddd)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rddd): There are repeated observations. Point\n#&gt; estimates and standard errors have been adjusted. Use option massPoints=FALSE\n#&gt; to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\n\n\nUsing a plot to confirm the argument.\n\n\n# Visually check continuity at running variable\nrdd_plot &lt;- rdplotdensity(rddd, df2$purchase_amount, plotN = 100)\n\n\n\n\n\n\n\n\nAs they did not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to obtain valid results."
  }
]